Got you, Chef. Here is a single cut and paste **bootstrap script** that generates the entire Anthropic enabled LocalAIApp, ready for Replit and GitHub, and built for macOS with Ollama offline by default.

Save this as `bootstrap_local_ai_app_v2.py` and run it once with `python bootstrap_local_ai_app_v2.py`. It will create the `LocalAIApp` folder with all files, including Anthropic mode, OpenAI mode, and Ollama offline mode.

````
# bootstrap_local_ai_app_v2.py
# Generates the complete LocalAIApp with PyQt6 GUI, SQLite history, Ollama offline mode,
# OpenAI online mode, Anthropic online mode, settings, exports, logs, and GitHub ready files.

import os, json, textwrap, sqlite3, sys, datetime

PROJECT = "LocalAIApp"

FILES = {
    ".gitignore": """
# Python
__pycache__/
*.pyc
.venv/
venv/
env/
.env
# macOS
.DS_Store
# Logs and build
logs/
*.log
dist/
build/
*.spec
    """,

    "requirements.txt": """
PyQt6==6.6.1
requests>=2.31.0
openai>=1.30.0
    """,

    "README.md": """
# LocalAIApp

Local desktop chat app for macOS using a local LLM with Ollama and optional online models through OpenAI and Anthropic.  
Runs offline by default. Switch providers in the Mode menu or the Settings dialog.

## Features
- PyQt6 desktop GUI
- Offline mode through Ollama at http://localhost:11434
- Online mode through OpenAI or Anthropic
- Streaming responses for smooth chat
- Unlimited chat history in SQLite
- Export conversations to JSON or TXT
- Settings in JSON for models and keys
- Logs folder placeholder

## Requirements
- Python 3.10 or newer
- macOS recommended
- Ollama installed and running for offline mode
- OpenAI API key and or Anthropic API key for online modes

## Quick Start

1) Create a virtual environment and install packages
```bash
cd LocalAIApp
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
````

2. Start Ollama locally then pull a model

```bash
brew install ollama   # if not installed
ollama serve
ollama pull llama3
```

3. Run the app

```bash
python main.py
```

4. Switch modes and set keys

* Use the Settings menu to choose mode and models and paste keys
* You can also set OPENAI_API_KEY and ANTHROPIC_API_KEY in your environment

## Packaging for macOS

Optional using PyInstaller

```bash
pip install pyinstaller
pyinstaller --windowed --name LocalAIApp --icon assets/app.icns main.py
```

## Project Structure

```
/LocalAIApp
├── main.py
├── gui.py
├── llm_client.py
├── db.py
├── settings.json
├── requirements.txt
├── assets/
│   └── app.png
├── logs/
└── README.md
```

## GitHub

```bash
cd LocalAIApp
git init
git add .
git commit -m "Initial commit for LocalAIApp with Ollama OpenAI Anthropic"
git branch -M main
git remote add origin https://github.com/<your-username>/LocalAIApp.git
git push -u origin main
```

```
""",

"main.py": r"""
```

import sys
from pathlib import Path
import json
from PyQt6.QtWidgets import QApplication
from gui import MainWindow
from db import Database

APP_DIR = Path(**file**).resolve().parent
SETTINGS_PATH = APP_DIR / "settings.json"
LOGS_DIR = APP_DIR / "logs"
LOGS_DIR.mkdir(exist_ok=True)

DEFAULT_SETTINGS = {
"mode": "offline",                         # offline, openai, anthropic
"offline_model": "llama3",
"online_model": "gpt-4o-mini",
"anthropic_model": "claude-3-opus-20240229",
"ollama_base_url": "[http://localhost:11434](http://localhost:11434)",
"openai_api_key": "",
"anthropic_api_key": "",
"temperature": 0.7,
"system_prompt": "You are a helpful assistant.",
"max_tokens": 1024
}

def ensure_settings():
if not SETTINGS_PATH.exists():
with open(SETTINGS_PATH, "w") as f:
json.dump(DEFAULT_SETTINGS, f, indent=2)

def main():
ensure_settings()
db = Database()
app = QApplication(sys.argv)
app.setApplicationName("LocalAIApp")
window = MainWindow(db, str(SETTINGS_PATH))
window.show()
sys.exit(app.exec())

if **name** == "**main**":
main()
""",

```
"gui.py": r"""
```

import json
from pathlib import Path
from PyQt6.QtCore import QThread, pyqtSignal
from PyQt6.QtGui import QAction, QIcon
from PyQt6.QtWidgets import (
QMainWindow, QWidget, QVBoxLayout, QTextEdit, QLineEdit, QPushButton,
QFileDialog, QMessageBox, QHBoxLayout, QLabel, QDialog, QFormLayout, QComboBox
)

from llm_client import LLMClient
from db import ensure_schema

ASSETS = Path(**file**).resolve().parent / "assets"

class StreamWorker(QThread):
chunk = pyqtSignal(str)
done = pyqtSignal(str)
error = pyqtSignal(str)

```
def __init__(self, client: LLMClient, session_id: str, messages: list, parent=None):
    super().__init__(parent)
    self.client = client
    self.session_id = session_id
    self.messages = messages

def run(self):
    try:
        full_text = ""
        for piece in self.client.stream_chat(self.messages, session_id=self.session_id):
            full_text += piece
            self.chunk.emit(piece)
        self.done.emit(full_text)
    except Exception as e:
        self.error.emit(str(e))
```

class SettingsDialog(QDialog):
def **init**(self, settings_path: str, parent=None):
super().**init**(parent)
self.setWindowTitle("Settings and Models and Keys")
self.settings_path = Path(settings_path)
with open(self.settings_path) as f:
self.settings = json.load(f)

```
    layout = QFormLayout(self)

    self.mode = QComboBox()
    self.mode.addItems(["offline", "openai", "anthropic"])
    self.mode.setCurrentText(self.settings.get("mode", "offline"))

    self.offline_model = QLineEdit(self.settings.get("offline_model", "llama3"))
    self.online_model = QLineEdit(self.settings.get("online_model", "gpt-4o-mini"))
    self.anthropic_model = QLineEdit(self.settings.get("anthropic_model", "claude-3-opus-20240229"))

    self.ollama_url = QLineEdit(self.settings.get("ollama_base_url", "http://localhost:11434"))
    self.openai_key = QLineEdit(self.settings.get("openai_api_key", ""))
    self.openai_key.setEchoMode(QLineEdit.EchoMode.Password)
    self.anthropic_key = QLineEdit(self.settings.get("anthropic_api_key", ""))
    self.anthropic_key.setEchoMode(QLineEdit.EchoMode.Password)

    self.temp = QLineEdit(str(self.settings.get("temperature", 0.7)))
    self.max_tok = QLineEdit(str(self.settings.get("max_tokens", 1024)))
    self.system_prompt = QTextEdit(self.settings.get("system_prompt", "You are a helpful assistant."))

    layout.addRow("Mode", self.mode)
    layout.addRow("Offline model", self.offline_model)
    layout.addRow("OpenAI model", self.online_model)
    layout.addRow("Anthropic model", self.anthropic_model)
    layout.addRow("Ollama URL", self.ollama_url)
    layout.addRow("OpenAI API Key", self.openai_key)
    layout.addRow("Anthropic API Key", self.anthropic_key)
    layout.addRow("Temperature", self.temp)
    layout.addRow("Max tokens", self.max_tok)
    layout.addRow("System Prompt", self.system_prompt)

    btns = QHBoxLayout()
    save = QPushButton("Save")
    cancel = QPushButton("Cancel")
    btns.addWidget(save)
    btns.addWidget(cancel)
    layout.addRow(btns)

    save.clicked.connect(self.save_settings)
    cancel.clicked.connect(self.reject)

def save_settings(self):
    try:
        self.settings["mode"] = self.mode.currentText()
        self.settings["offline_model"] = self.offline_model.text().strip()
        self.settings["online_model"] = self.online_model.text().strip()
        self.settings["anthropic_model"] = self.anthropic_model.text().strip()
        self.settings["ollama_base_url"] = self.ollama_url.text().strip()
        self.settings["openai_api_key"] = self.openai_key.text().strip()
        self.settings["anthropic_api_key"] = self.anthropic_key.text().strip()
        self.settings["temperature"] = float(self.temp.text().strip())
        self.settings["max_tokens"] = int(self.max_tok.text().strip())
        self.settings["system_prompt"] = self.system_prompt.toPlainText().strip()
        with open(self.settings_path, "w") as f:
            json.dump(self.settings, f, indent=2)
        self.accept()
    except Exception as e:
        QMessageBox.critical(self, "Error", str(e))
```

class MainWindow(QMainWindow):
def **init**(self, db, settings_path: str):
super().**init**()
self.db = db
ensure_schema(self.db.conn)
self.settings_path = Path(settings_path)
with open(self.settings_path) as f:
self.settings = json.load(f)

```
    self.setWindowTitle("LocalAIApp")
    icon_path = ASSETS / "app.png"
    if icon_path.exists():
        self.setWindowIcon(QIcon(str(icon_path)))

    self.session_id = self.db.start_new_session()

    central = QWidget()
    self.setCentralWidget(central)
    v = QVBoxLayout(central)

    self.chat_view = QTextEdit()
    self.chat_view.setReadOnly(True)
    self.input = QLineEdit()
    self.input.setPlaceholderText("Type your message...")
    self.send_btn = QPushButton("Send")

    row = QHBoxLayout()
    row.addWidget(self.input)
    row.addWidget(self.send_btn)

    v.addWidget(self.chat_view)
    v.addLayout(row)

    self.status_lbl = QLabel("Mode: " + self.settings.get("mode", "offline"))
    v.addWidget(self.status_lbl)

    self.send_btn.clicked.connect(self.on_send)

    menubar = self.menuBar()
    m_app = menubar.addMenu("App")
    m_mode = menubar.addMenu("Mode")
    m_history = menubar.addMenu("History")
    m_export = menubar.addMenu("Export")
    m_settings = menubar.addMenu("Settings")

    quit_act = QAction("Quit", self)
    quit_act.triggered.connect(self.close)
    m_app.addAction(quit_act)

    offline_act = QAction("Switch to Offline", self)
    openai_act = QAction("Switch to OpenAI", self)
    anthropic_act = QAction("Switch to Anthropic", self)
    offline_act.triggered.connect(lambda: self.switch_mode("offline"))
    openai_act.triggered.connect(lambda: self.switch_mode("openai"))
    anthropic_act.triggered.connect(lambda: self.switch_mode("anthropic"))
    m_mode.addAction(offline_act)
    m_mode.addAction(openai_act)
    m_mode.addAction(anthropic_act)

    view_hist = QAction("View Session History", self)
    view_hist.triggered.connect(self.view_history)
    m_history.addAction(view_hist)

    export_json = QAction("Export Session as JSON", self)
    export_txt = QAction("Export Session as TXT", self)
    export_json.triggered.connect(lambda: self.export_session("json"))
    export_txt.triggered.connect(lambda: self.export_session("txt"))
    m_export.addAction(export_json)
    m_export.addAction(export_txt)

    settings_act = QAction("Models and API Keys", self)
    settings_act.triggered.connect(self.open_settings)
    m_settings.addAction(settings_act)

    self.client = LLMClient(self.settings, self.db)
    self.load_session_into_view()

def load_session_into_view(self):
    messages = self.db.get_messages(self.session_id)
    self.chat_view.clear()
    for m in messages:
        self.append_message(m["role"], m["content"])

def append_message(self, role: str, content: str):
    if role == "user":
        self.chat_view.append(f"<p><b>You:</b> {content}</p>")
    else:
        self.chat_view.append(f"<p><b>Assistant:</b> {content}</p>")

def on_send(self):
    text = self.input.text().strip()
    if not text:
        return
    self.input.clear()
    self.db.add_message(self.session_id, "user", text)
    self.append_message("user", text)

    messages = self.db.get_messages(self.session_id, as_openai_format=True)
    self.stream_thread = StreamWorker(self.client, self.session_id, messages)
    self.stream_thread.chunk.connect(self._on_stream_chunk)
    self.stream_thread.done.connect(self._on_stream_done)
    self.stream_thread.error.connect(self._on_stream_error)
    self.stream_thread.start()

def _on_stream_chunk(self, chunk: str):
    self._ensure_last_assistant_paragraph()
    self._append_to_last_assistant(chunk)

def _on_stream_done(self, full: str):
    self.db.add_message(self.session_id, "assistant", full)

def _on_stream_error(self, err: str):
    QMessageBox.critical(self, "Error", err)

def _ensure_last_assistant_paragraph(self):
    cursor = self.chat_view.textCursor()
    cursor.movePosition(cursor.MoveOperation.End)
    html = self.chat_view.toHtml()
    if "Assistant:" not in html[-300:]:
        self.chat_view.append(f"<p><b>Assistant:</b> </p>")

def _append_to_last_assistant(self, text: str):
    cursor = self.chat_view.textCursor()
    cursor.movePosition(cursor.MoveOperation.End)
    cursor.insertText(text)
    self.chat_view.setTextCursor(cursor)

def switch_mode(self, mode: str):
    self.settings["mode"] = mode
    with open(self.settings_path, "w") as f:
        json.dump(self.settings, f, indent=2)
    self.status_lbl.setText("Mode: " + mode)
    self.client = LLMClient(self.settings, self.db)

def open_settings(self):
    dlg = SettingsDialog(str(self.settings_path), self)
    if dlg.exec():
        with open(self.settings_path) as f:
            self.settings = json.load(f)
        self.status_lbl.setText("Mode: " + self.settings.get("mode", "offline"))
        self.client = LLMClient(self.settings, self.db)

def view_history(self):
    msgs = self.db.get_messages(self.session_id)
    lines = []
    for m in msgs:
        lines.append(f"[{m['created_at']}] {m['role']}: {m['content']}")
    dlg = QDialog(self)
    dlg.setWindowTitle("Session History")
    lay = QVBoxLayout(dlg)
    box = QTextEdit()
    box.setReadOnly(True)
    box.setPlainText("\n\n".join(lines))
    lay.addWidget(box)
    close = QPushButton("Close")
    close.clicked.connect(dlg.accept)
    lay.addWidget(close)
    dlg.exec()

def export_session(self, kind: str):
    if kind == "json":
        path, _ = QFileDialog.getSaveFileName(self, "Save JSON", "conversation.json", "JSON Files (*.json)")
        if not path:
            return
        data = self.db.get_messages(self.session_id)
        with open(path, "w") as f:
            json.dump(data, f, indent=2)
        QMessageBox.information(self, "Export", "Exported as JSON")
    else:
        path, _ = QFileDialog.getSaveFileName(self, "Save TXT", "conversation.txt", "Text Files (*.txt)")
        if not path:
            return
        data = self.db.get_messages(self.session_id)
        with open(path, "w") as f:
            for m in data:
                f.write(f"[{m['created_at']}] {m['role'].upper()}: {m['content']}\n\n")
        QMessageBox.information(self, "Export", "Exported as TXT")
""",

"llm_client.py": r"""
```

import os
import json
import requests
from typing import Generator, List, Dict

class LLMClient:
def **init**(self, settings: dict, db):
self.settings = settings
self.db = db
self.mode = settings.get("mode", "offline")
self.ollama_base = settings.get("ollama_base_url", "[http://localhost:11434](http://localhost:11434)")
self.offline_model = settings.get("offline_model", "llama3")
self.online_model = settings.get("online_model", "gpt-4o-mini")
self.anthropic_model = settings.get("anthropic_model", "claude-3-opus-20240229")
self.temperature = settings.get("temperature", 0.7)
self.max_tokens = settings.get("max_tokens", 1024)
self.system_prompt = settings.get("system_prompt", "You are a helpful assistant.")
self.openai_key = os.getenv("OPENAI_API_KEY", settings.get("openai_api_key", ""))
self.anthropic_key = os.getenv("ANTHROPIC_API_KEY", settings.get("anthropic_api_key", ""))

```
def stream_chat(self, messages: List[Dict], session_id: str) -> Generator[str, None, None]:
    if self.mode == "offline":
        yield from self._ollama_stream(messages)
    elif self.mode == "anthropic":
        yield from self._anthropic_stream(messages)
    else:
        yield from self._openai_stream(messages)

def _with_system(self, messages: List[Dict]) -> List[Dict]:
    has_system = any(m.get("role") == "system" for m in messages)
    if has_system:
        return messages
    return [{"role": "system", "content": self.system_prompt}] + messages

# Ollama chat streaming
def _ollama_stream(self, messages: List[Dict]) -> Generator[str, None, None]:
    url = f"{self.ollama_base}/api/chat"
    payload = {
        "model": self.offline_model,
        "messages": self._with_system(messages),
        "stream": True,
        "options": {"temperature": self.temperature}
    }
    with requests.post(url, json=payload, stream=True, timeout=600) as r:
        r.raise_for_status()
        for line in r.iter_lines(decode_unicode=True):
            if not line:
                continue
            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                continue
            msg = data.get("message", {})
            chunk = msg.get("content", "")
            if chunk:
                yield chunk

# OpenAI chat streaming
def _openai_stream(self, messages: List[Dict]) -> Generator[str, None, None]:
    if not self.openai_key:
        raise ValueError("OpenAI API key is missing. Set it in settings or environment.")
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {self.openai_key}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": self.online_model,
        "messages": self._with_system(messages),
        "temperature": self.temperature,
        "max_tokens": self.max_tokens,
        "stream": True
    }
    with requests.post(url, headers=headers, json=payload, stream=True, timeout=600) as r:
        r.raise_for_status()
        for line in r.iter_lines(decode_unicode=True):
            if not line or not line.startswith("data:"):
                continue
            data_str = line.replace("data: ", "").strip()
            if data_str == "[DONE]":
                break
            try:
                data = json.loads(data_str)
            except json.JSONDecodeError:
                continue
            delta = data.get("choices", [{}])[0].get("delta", {})
            piece = delta.get("content", "")
            if piece:
                yield piece

# Anthropic chat streaming
def _anthropic_stream(self, messages: List[Dict]) -> Generator[str, None, None]:
    if not self.anthropic_key:
        raise ValueError("Anthropic API key is missing. Set it in settings or environment.")
    url = "https://api.anthropic.com/v1/messages"
    headers = {
        "x-api-key": self.anthropic_key,
        "anthropic-version": "2023-06-01",
        "content-type": "application/json"
    }
    # Convert OpenAI style to Anthropic messages
    sys_prompt = None
    user_turns = []
    for m in messages:
        role = m.get("role")
        content = m.get("content", "")
        if role == "system":
            sys_prompt = content
        elif role == "user":
            user_turns.append({"role": "user", "content": content})
        elif role == "assistant":
            user_turns.append({"role": "assistant", "content": content})

    if sys_prompt is None:
        sys_prompt = self.system_prompt

    payload = {
        "model": self.anthropic_model,
        "max_tokens": self.max_tokens,
        "temperature": self.temperature,
        "system": sys_prompt,
        "stream": True,
        "messages": [{"role": u["role"], "content": u["content"]} for u in user_turns]
    }

    with requests.post(url, headers=headers, json=payload, stream=True, timeout=600) as r:
        r.raise_for_status()
        for raw in r.iter_lines(decode_unicode=True):
            if not raw:
                continue
            # SSE lines may include "event:" and "data:" lines
            if not raw.startswith("data:"):
                continue
            data_str = raw.replace("data:", "", 1).strip()
            if data_str == "[DONE]":
                break
            try:
                ev = json.loads(data_str)
            except json.JSONDecodeError:
                continue
            # Anthropic events can be:
            # message_start, content_block_start, content_block_delta, message_delta, content_block_stop, message_stop
            # We want text deltas
            # content_block_delta => {"type":"content_block_delta","delta":{"type":"text_delta","text":"..."}}
            if ev.get("type") == "content_block_delta":
                delta = ev.get("delta", {})
                txt = delta.get("text", "")
                if txt:
                    yield txt
            # Some server implementations use {"delta":{"type":"text_delta","text":"..."}} at top level
            elif "delta" in ev and isinstance(ev["delta"], dict):
                txt = ev["delta"].get("text", "")
                if txt:
                    yield txt
""",

"db.py": r"""
```

import sqlite3
import uuid
import datetime

def utc_now_iso():
return datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"

class Database:
def **init**(self, path: str = "chat_history.db"):
self.conn = sqlite3.connect(path)
self.conn.row_factory = sqlite3.Row
ensure_schema(self.conn)

```
def start_new_session(self) -> str:
    session_id = str(uuid.uuid4())
    cur = self.conn.cursor()
    cur.execute(
        "INSERT INTO sessions (id, created_at) VALUES (?, ?)",
        (session_id, utc_now_iso())
    )
    self.conn.commit()
    return session_id

def add_message(self, session_id: str, role: str, content: str):
    cur = self.conn.cursor()
    cur.execute(
        "INSERT INTO messages (session_id, role, content, created_at) VALUES (?, ?, ?, ?)",
        (session_id, role, content, utc_now_iso())
    )
    self.conn.commit()
```
